# serving/core/pipelines/dashboard_generator.py
import logging
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from google.cloud import bigquery
from typing import Dict, Any, Optional

from .. import config, gcs

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - [%(threadName)s] - %(message)s")
PREP_PREFIX = "prep/"
OUTPUT_PREFIX = "dashboards/"
PAGE_JSON_PREFIX = "pages/"
PRICE_CHART_JSON_FOLDER = "price-chart-json/"
MAX_WORKERS = 16 # Increased workers for a simpler, faster I/O bound task

def _delete_old_dashboard_files(ticker: str):
    """Deletes all previous dashboard JSON files for a given ticker."""
    prefix = f"{OUTPUT_PREFIX}{ticker}_dashboard_"
    blobs_to_delete = gcs.list_blobs(config.GCS_BUCKET_NAME, prefix)
    for blob_name in blobs_to_delete:
        try:
            gcs.delete_blob(config.GCS_BUCKET_NAME, blob_name)
        except Exception as e:
            logging.error(f"[{ticker}] Failed to delete old dashboard file {blob_name}: {e}")

def _get_company_metadata(ticker: str) -> Dict[str, Any]:
    """Fetches basic company metadata from BigQuery."""
    client = bigquery.Client(project=config.SOURCE_PROJECT_ID)
    query = f"SELECT company_name FROM `{config.BUNDLER_STOCK_METADATA_TABLE_ID}` WHERE ticker = @ticker ORDER BY quarter_end_date DESC LIMIT 1"
    job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter("ticker", "STRING", ticker)])
    df = client.query(query, job_config=job_config).to_dataframe()
    return df.iloc[0].to_dict() if not df.empty else {"company_name": ticker}

def _get_price_chart_data(ticker: str) -> Optional[Dict[str, Any]]:
    """Fetches the latest price chart JSON file for a ticker."""
    latest_blob = gcs.get_latest_blob_for_ticker(config.GCS_BUCKET_NAME, PRICE_CHART_JSON_FOLDER, ticker)
    if latest_blob:
        try:
            return json.loads(latest_blob.download_as_text())
        except (json.JSONDecodeError, Exception) as e:
            logging.error(f"[{ticker}] Failed to read or parse price chart JSON: {e}")
    return None

def _get_page_json(ticker: str, run_date: str) -> Optional[Dict[str, Any]]:
    """
    Fetches the SEO/Analyst content JSON generated by page_generator.py.
    """
    blob_name = f"{PAGE_JSON_PREFIX}{ticker}_page_{run_date}.json"
    try:
        content = gcs.read_blob(config.GCS_BUCKET_NAME, blob_name)
        if content:
            return json.loads(content)
    except Exception as e:
        logging.warning(f"[{ticker}] SEO Page JSON not found or invalid at {blob_name}: {e}")
    return None

def process_prep_file(prep_blob_name: str) -> Optional[str]:
    """
    Processes a single prep file to generate a merged Unified Dashboard JSON.
    """
    match = re.search(r'prep/([A-Z\.]+)_(\d{4}-\d{2}-\d{2})\.json$', prep_blob_name)
    if not match:
        logging.warning(f"Could not parse ticker/date from prep file name: {prep_blob_name}. Skipping.")
        return None
    
    ticker, run_date_str = match.groups()
    logging.info(f"[{ticker}] Starting dashboard generation from {prep_blob_name}...")

    try:
        prep_json_str = gcs.read_blob(config.GCS_BUCKET_NAME, prep_blob_name)
        if not prep_json_str:
            logging.warning(f"[{ticker}] SKIPPING: Prep file is empty.")
            return None
        prep_data = json.loads(prep_json_str)

        if not prep_data.get("kpis"):
            logging.warning(f"[{ticker}] SKIPPING: Prep file is missing 'kpis' data.")
            return None

        metadata = _get_company_metadata(ticker)
        company_name = metadata.get("company_name", ticker)
        
        # --- NEW: Fetch and Merge SEO/Analyst Content ---
        page_json = _get_page_json(ticker, run_date_str)
        
        analysis_section = {}
        seo_section = {}
        
        if page_json:
            # 1. Summary (Execution Deck)
            analysis_section["summary"] = {
                "signal": page_json.get("tradeSetup", {}).get("signal", "Neutral"),
                "score": page_json.get("bullishScore", 50),
                "confidence": page_json.get("tradeSetup", {}).get("confidence", "Medium")
            }
            
            # 2. Options Brief (Tab 1 - The New Alpha)
            # Merge the HTML content with the raw Market Structure data
            options_brief = page_json.get("analystBrief", {})
            options_brief["marketStructure"] = page_json.get("marketStructure", {})
            analysis_section["optionsBrief"] = options_brief
            
            # 3. Fundamental Thesis (Tab 2 - Context)
            # Use 'contentBlocks.thesis' if available, fallback to raw fullAnalysis text
            content_blocks = page_json.get("contentBlocks", {})
            full_analysis = page_json.get("fullAnalysis", {})
            
            # Construct a rich thesis object
            thesis_content = content_blocks.get("thesis")
            if not thesis_content and full_analysis:
                # Fallback: Merge News + Fundamentals if specific thesis block is missing
                thesis_content = f"<h3>Recent Developments</h3><p>{full_analysis.get('news', '')}</p><h3>Fundamental Outlook</h3><p>{full_analysis.get('fundamentals', '')}</p>"
                
            analysis_section["fundamentalThesis"] = {
                "headline": "Macro & Fundamental Drivers", 
                "content": thesis_content,
                "catalysts": content_blocks.get("catalysts", page_json.get("tradeSetup", {}).get("catalyst", []))
            }
            if isinstance(analysis_section["fundamentalThesis"]["catalysts"], str):
                 analysis_section["fundamentalThesis"]["catalysts"] = [analysis_section["fundamentalThesis"]["catalysts"]]

            # 4. Trade Plan
            analysis_section["tradeSetup"] = page_json.get("tradeSetup", {})
            
            # 5. SEO Metadata
            seo_section = page_json.get("seo", {})
        else:
            # Fallback if SEO page generation failed or hasn't run yet
            analysis_section = {
                "summary": {"signal": "Pending", "score": 50, "confidence": "Low"},
                "optionsBrief": {"headline": "Analysis Pending", "content": "<p>Options data is currently processing.</p>"},
                "fundamentalThesis": {"headline": "Analysis Pending", "content": "<p>Fundamental data is processing.</p>"},
                "tradeSetup": {}
            }

        # Assemble the final dashboard
        final_dashboard = {
            "ticker": ticker,
            "runDate": run_date_str,
            "titleInfo": {"companyName": company_name, "ticker": ticker, "asOfDate": run_date_str},
            "kpis": prep_data.get("kpis"),
            "priceChartData": _get_price_chart_data(ticker),
            "analysis": analysis_section,
            "seo": seo_section
        }
        
        _delete_old_dashboard_files(ticker)
        output_blob_name = f"{OUTPUT_PREFIX}{ticker}_dashboard_{run_date_str}.json"
        gcs.write_text(config.GCS_BUCKET_NAME, output_blob_name, json.dumps(final_dashboard, indent=2))
        logging.info(f"[{ticker}] SUCCESS: Generated Unified Dashboard JSON.")
        return output_blob_name

    except Exception as e:
        logging.error(f"[{ticker}] CRITICAL ERROR during dashboard generation from {prep_blob_name}: {e}", exc_info=True)
        return None

def run_pipeline():
    """
    Main pipeline to generate a dashboard for every available prep file.
    """
    logging.info("--- Starting Dashboard Generation Pipeline (Ultra-Simplified) ---")
    
    work_items = gcs.list_blobs(config.GCS_BUCKET_NAME, prefix=PREP_PREFIX)
    if not work_items:
        logging.warning("No prep files found to process. Exiting.")
        return

    logging.info(f"Found {len(work_items)} prep files to process into dashboards.")
    processed_count = 0
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_item = {executor.submit(process_prep_file, item): item for item in work_items}
        for future in as_completed(future_to_item):
            try:
                if future.result():
                    processed_count += 1
            except Exception as exc:
                logging.error(f"Prep file {future_to_item[future]} caused an unhandled exception: {exc}", exc_info=True)
    
    logging.info(f"--- Dashboard Generation Pipeline Finished. Successfully generated {processed_count} of {len(work_items)} dashboards. ---")

if __name__ == "__main__":
    run_pipeline()